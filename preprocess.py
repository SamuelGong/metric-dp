# -*- coding: utf-8 -*-
"""Dataset Privatizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NlkHnzenDDTSB3dNbvMEGds9-iulT18f
"""

!pip install --quiet transformers
!pip install --quiet datasets
!pip install --quiet annoy

import math
import torch
import pickle
import itertools
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm
import matplotlib.pyplot as plt
sns.set_theme(context='paper',
              style='ticks',
              palette='tab10',
              rc={"axes.grid": True,
                  "grid.linestyle": 'dashed',
                  "axes.linewidth": 1.0,
                  "axes.facecolor": '1.0',
                  }
              )
import collections

def load_pickle(fname):
    with open(fname, "rb") as f:
        return pickle.load(f)

def dump_pickle(file, fname):
    with open(fname, "wb") as f:
        pickle.dump(file, f)

device = ('cuda' if torch.cuda.is_available() else 'cpu')
device

from annoy import AnnoyIndex

class metricDP():
   
    def __init__(self, start_from=999):

        '''
        Code in part from Amazon SageMaker, Vocabular [Dictionary] is a token to
        index mapping, Embedding [Array] including special tokens such as [UNK],
        [PAD], [CLS], [SEP], [MASK], or [unused...]. Code expects special tokens
        at the front and regular tokens continuing from 'start_from'. Parameters
        defaulted to BERT (base, uncased).
        '''
        from transformers import BertTokenizer, BertModel
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.encoder = BertModel.from_pretrained("bert-base-uncased").to(device)

        self.vocabular = self.tokenizer.vocab
        self.embedding = self.encoder.embeddings.word_embeddings.weight.cpu().detach().numpy()

        self.vocab_size = self.embedding.shape[0]
        self.embed_dim = self.embedding.shape[1]

        self.start_from = start_from

    def build_ann(self, metric='euclidean', n_trees=50):

        ''' Build Approximate Nearest Neighbors, excluding special tokens '''
        
        self.ann = AnnoyIndex(self.embed_dim, metric)

        for index, vector in enumerate(self.embedding[self.start_from:,:]):
            self.ann.add_item(index, vector)
            
        self.ann.build(n_trees)
        
    
    def privatize(self, tokens, epsilon=10, modus='lexical'):
        
        if modus == 'lexical':
            
            #tokens = self.tokenizer.tokenize(sentence)
            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)
            token_vec = np.array([self.embedding[ids] for ids in token_ids])

        elif modus == 'contextual':
            
            with torch.no_grad():
            
                inputs = self.tokenizer.encode_plus(
                    text=tokens,
                    is_split_into_words=True,
                    truncation=True,
                    padding='max_length',
                    max_length=512,
                    return_tensors='pt',
                )
                
                length = torch.count_nonzero(inputs['input_ids'])
                
                inputs = {k:v.to(device) for k,v in inputs.items()}
                
                token_vec = self.encoder(**inputs)[
                    'last_hidden_state'
                ].squeeze(0)[1:length-1,:].cpu().numpy()
                
        def replace(vector, epsilon):
        
              random_vec = np.random.normal(size=self.embed_dim)
              normalized_vec = random_vec / np.linalg.norm(random_vec)
              magnitude = np.random.gamma(shape=self.embed_dim, scale=1/epsilon)
              noise = normalized_vec * magnitude
              noisy_vector = vector + noise
              return self.ann.get_nns_by_vector(noisy_vector, 1)[0]
          
        assert self.ann != None, 'Build or Init ANNs.'
        
        tokens = []
        for index, vector in enumerate(token_vec):
                tokens.append( replace(vector, epsilon) + self.start_from)

        return self.tokenizer.decode(tokens), self.tokenizer.convert_ids_to_tokens(tokens)

mdp = metricDP(start_from=999)
mdp.build_ann(metric='euclidean', n_trees=50)

"""#Preprocessing"""

import datasets
datasets.disable_caching() 

from datasets import load_dataset
train_data = load_dataset('imdb', split='train')
valid_data = load_dataset('imdb', split='test')

import re
import string

from bs4 import BeautifulSoup

#Removing the html strips
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

#Defining clean_text function
def clean(example):
    text = example['text']
    text = strip_html(text)
    text = re.sub(r'[^A-Za-z0-9]+',' ',text)
    text = text.lower()
    example['text'] = text
    return example

train_data[0]

"""
import datasets
datasets.disable_caching()

from datasets import load_dataset
train_data = load_dataset('ag_news', split='train')
valid_data = load_dataset('ag_news', split='test')

import re

def clean(example):
    text = example['text']
    text = re.sub('\\\\', ' ', text)
    text = re.sub(' +', ' ', text)
    text = re.sub('\'\'', '"', text)
    text = re.sub('/', '', text)
    text = re.sub('\``', '"', text)
    example['text'] = text
    return example

#train_data = train_data.select(range(100))
train_data = train_data.map(lambda i: clean(i))
#valid_data = valid_data.select(range(100))
valid_data = valid_data.map(lambda i: clean(i))

train_data[0]
"""
from transformers import BertTokenizerFast
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

def tokenize(example):

  example['tokens'] = tokenizer.tokenize(example['text'])
  return example

train_data = train_data.map(lambda i: tokenize(i))
valid_data = valid_data.map(lambda i: tokenize(i))

train_data.save_to_disk(f"train_imdb")
valid_data.save_to_disk(f"valid_imdb")

# Commented out IPython magic to ensure Python compatibility.
# %pprint

train_data[0]

MODUS = 'contextual'
EPSILON = 25

def privatize(example, epsilon=10, modus='lexical'):

    text, tokens = mdp.privatize(
        example['tokens'],
        epsilon=epsilon,
        modus=modus
    )

    example['text'] = text
    example['tokens'] = tokens

    return example

train_data_priv = train_data.map(lambda i: privatize(i, epsilon=EPSILON, modus=MODUS))
train_data_priv.save_to_disk(f"train_imdb_{MODUS}_{EPSILON}")